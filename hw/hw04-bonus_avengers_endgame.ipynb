{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9074d943",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/r_probability/master/end_seminars_2020/sem08/real_expect.png\" width=\"500\">\n",
    "\n",
    "# Андан на экономе: домашнее задание 4\n",
    "</center>\n",
    "\n",
    "\n",
    "> Если орел, я выиграла. Если решка, ты проиграл.\n",
    "\n",
    "$\\qquad$ $\\qquad$ $\\qquad$ $\\qquad$  **[Рейчел из Друзей](https://www.youtube.com/watch?v=TV9ghItJ2Ms)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153348a7",
   "metadata": {},
   "source": [
    "**ФИО:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40dc119",
   "metadata": {},
   "source": [
    "## Общая информация\n",
    "\n",
    "__Дата выдачи:__ 20.05.2024\n",
    "\n",
    "__Дедлайн:__ 23:59MSK 10.06.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b3815",
   "metadata": {},
   "source": [
    "## Часть 2: бонусная задача \n",
    "\n",
    "Эта часть домашки основана на рабочей задаче одного из семинаристов. За неё можно получить 20 бонусных баллов.\n",
    "\n",
    "Это задание будет довольно хардкорным, но интересным. Первые две задачи подготовительные. Информация из них пригодится вам при решении третьей задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cbdbe",
   "metadata": {},
   "source": [
    "### Задача 1: Доверительные интервалы для долей (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56665b52",
   "metadata": {},
   "source": [
    "В этой задаче вам предстоит поработать с разными видами доверительных интервалов для долей.\n",
    "\n",
    "Пусть случайные величины $X_1, \\ldots X_n$ независимы и имеют распределение Бернулли с неизвестным параметром $p$, при этом $\\hat p = \\bar x$. \n",
    "\n",
    "__а) [1 балл]__ Если воспользоваться ЦПТ, можно построить для неизвестного $p$ доверительный интервал Вальда. На лекции мы для этого использовали сходимость \n",
    "$$\n",
    "\\frac{\\hat p - p}{\\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}} \\to N(0;1),\n",
    "$$\n",
    "\n",
    "а затем решали неравенство \n",
    "$$\n",
    "-z_{cr} \\leq \\frac{\\hat p - p}{\\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}} \\leq z_{cr}.\n",
    "$$\n",
    "\n",
    "Получалось, что при уровне значимости $\\alpha$ доверительный интервал \n",
    "\n",
    "$$\n",
    "\\hat p \\pm z_{1 - \\frac{\\alpha}{2}} \\cdot \\sqrt{ \\frac{\\hat p \\cdot (1 - \\hat p)}{n} }\n",
    "$$\n",
    "\n",
    "накрывает неизвестное значение $p$ с вероятностью $1 - \\alpha$. Мы в этом задании с помощью симуляций на компьютере сравним фактическую вероятность накрытия неизвестного параметра $p$ интервалами Вальда, Вильсона и Агрести—Коуллаc номинальной 95\\%-й вероятностью. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52b0e8",
   "metadata": {},
   "source": [
    "### Сеттинг симуляций:\n",
    "\n",
    "1. Зафиксируем уровень значимости равный $5\\%$\n",
    "2. Будем перебирать $p$ в интервале $(0, 1)$ с шагом в 0.01\n",
    "3. Для каждого p проведем $1000$ симуляций\n",
    "    - генерируем с заданным $p$ выборку размера $n = 10$\n",
    "    - строим доверительный интервал для $p$\n",
    "    - cмотрим попадает ли истинное значение в доверительный интервал\n",
    "4. Оцениваем по симуляциям получившийся уровень значимости (доля случаев, когда доверительный интервал не покрыл $p$).\n",
    "5. Рисуем картинку, где по оси абсцис отложены значения $p$, а по оси ординат полученная оценка уровня значимости. Отдельной пунктирной линией рисуем на картинке $0.05$. \n",
    "\n",
    "Картинка должна получиться примерно такой же как [в этой статье.](https://www.ime.usp.br/~jmsinger/Textos/Castroetal2019.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fd7177",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf78bc",
   "metadata": {},
   "source": [
    "__б) [1 балл]__ Можно не заменять в дисперсии значения $p$ на $\\hat p$, а вместо этого воспользоваться сходимостью \n",
    "\n",
    "$$\n",
    "\\frac{\\hat p - p}{\\sqrt{\\frac{p (1 - p)}{n}}} \\to N(0;1)\n",
    "$$\n",
    "\n",
    "и решить относительно неизвестного $p$ (о ужас!) квадратное неравенство. Выйдет доверительный интервал Уилсона: \n",
    "\n",
    "$$\n",
    "\\hat p_w \\pm z_{1 - \\frac{\\alpha}{2}} \\cdot \\sqrt{\\frac{u \\hat p (1 - \\hat p) + (1 - u) (1/2)^2 }{n_w}},\n",
    "$$\n",
    "где $\\quad$ $\\hat p_w = u \\hat p + (1 - u) (1/2)$, $\\quad$  $u = \\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}$, $\\quad$ $(1-u) = \\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}$,  $\\quad$\n",
    "  $n_w = n + z_{1 - \\frac{\\alpha}{2}}^2$.\n",
    "\n",
    "**Необязательно:** с помощью ручки и бумаги убедитесь, что эти формулы верны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93c2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07993a",
   "metadata": {},
   "source": [
    "Повторите для доверительного интервала Уилсона тот же самый эксперимент и постройте картинку с поведением уровня значимости в зависимости от значений $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9278f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c95825",
   "metadata": {},
   "source": [
    "### Блок необязательных заданий\n",
    "\n",
    "> Эти задания вы можете попытаться решить на бумаге сами. Если вам лень это делать, достаточно их прочитать и как следует осмыслить ответы. \n",
    "\n",
    "**Задание 1:** \n",
    "\n",
    "Обозначим центр интервала Вильсона с помощью $\\hat p_w$. С помощью ручки и  бумаги докажите, что центр интервала Вильсона $\\hat p_w$ можно представить как средневзвешенное классической оценки $\\hat p$ и тривиальной оценки $1/2$,\n",
    "\n",
    "$$\n",
    "\\hat p_w = u \\cdot \\hat p + (1 - u) \\cdot (1/2).  \n",
    "$$\n",
    "\n",
    "Найдите веса $u$ и $(1-u)$.\n",
    "\n",
    "**Ответ:**  $u = \\frac{n}{n + z_{cr}^2}$, $\\quad$ $(1-u) = \\frac{z_{cr}^2}{n + z_{cr}^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b7c72",
   "metadata": {},
   "source": [
    "**Задание 2:**  Докажите, что центр интервала Вильсона $\\hat p_w$ можно проинтерпретировать следующим образом: добавим $f$ вымышленных $1$ и $f$ вымышленных $0$ в выборку и посчитаем классическую оценку вероятности для выборки с вымышленными наблюдениями,\n",
    "\n",
    "$$\n",
    "\\hat p_w = \\frac{\\sum_{i=1}^n Y_i + f}{n + 2 f}.\n",
    "$$\n",
    "\n",
    "Какому целому числу примерно равно $f$ для 95\\%-го доверительного интервала?\n",
    "\n",
    "__Ответ:__ \n",
    "\n",
    "$$\n",
    "\\hat p_w = \\frac{\\sum Y_i + z+{cr}^2/2}{n + z_{cr}^2},\n",
    "$$\n",
    "\n",
    "то есть мы добавляем в выборку $f = z_{cr}^2/2$ вымышленных $1$ и столько же вымышленных $0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40641826",
   "metadata": {},
   "source": [
    "**Задание 3:**  Докажите, что интервал Вильсона можно записать в виде\n",
    "\n",
    "$$\n",
    "\\hat p_w \\pm z_{cr} \\cdot \\sqrt{\\frac{u \\cdot \\hat p (1 - \\hat p) + (1 - u) \\cdot (1/2)^2 }{n_w}}.\n",
    "$$\n",
    "\n",
    "Найдите $n_w$, а также веса $u$ и $(1 - u)$.\n",
    "\n",
    "Таким образом, интервал Вильсона слегка корректирует число наблюдений и использует в качестве оценки дисперсии $X_i$ средневзвешенное между классической оценкой $\\hat p (1 - \\hat p)$ и тривиальной оценкой $1/4$.\n",
    "\n",
    "__Ответ:__\n",
    "\n",
    "$\\hat p_w = u \\cdot \\hat p + (1 - u) \\cdot (1/2)$, $\\quad$ $u = \\frac{n}{n + z_{cr}^2}$, $\\quad$  $(1-u) = \\frac{z_{cr}^2}{n + z_{cr}^2}$, $\\quad$ $n_w = n + z_{cr}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4737298",
   "metadata": {},
   "source": [
    "__в) [1 балл]__ Доверительный интервал Агрести—Коулла для уровня доверия 95\\% строится следующим образом.\n",
    "В выборку мысленно добавляют два наблюдения равных $1$ и два наблюдений равных $0$,\n",
    "считают оценку доли\n",
    "$$\n",
    "\\hat p_{ac} = \\frac{\\sum_{i=1}^n Y_i + 2}{n + 4},  \n",
    "$$ \n",
    "а затем строят классический интервал Вальда, используя $\\hat p_{ac}$ вместо классической $\\hat p$. \n",
    "\n",
    "Повторите для доверительного интервала Агрести—Коулла тот же самый эксперимент и постройте картинку с поведением уровня значимости в зависимости от значений $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa20c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a918892",
   "metadata": {},
   "source": [
    "__г) [1 балл]__ С помощью [метода стабилизации дисперсии](https://youtu.be/JGNwZKwE2h0) для $p$ можно получить следующий доверительный интервал: \n",
    "\n",
    "$$\n",
    "\\sin^2 \\left(\\arcsin \\sqrt{\\hat p} \\pm \\frac{z_{cr}}{2 \\sqrt{n}} \\right).\n",
    "$$\n",
    "\n",
    "Проведите для него тот же самый эксперимент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374e07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5345ae3",
   "metadata": {},
   "source": [
    "__д) [1 балл]__ Дайте развёрнутый ответ на следующие вопросы.\n",
    "\n",
    "1. Правда ли, что при уровне доверия 95\\% центры интервала Агрести~— Коулла и Вильсона совпадают?\n",
    "\n",
    "__Ответ:__ \n",
    "\n",
    "2. Какой 95\\%-й интервал шире, Агрести—Коулла или Вильсона?\n",
    "\n",
    "__Ответ:__ \n",
    "\n",
    "3. Какой 95\\%-й интервал шире, Вальда или основанный на стабилизации дисперсии?\n",
    "\n",
    "__Ответ:__ \n",
    "\n",
    "4. Как доверительные интервалы ведут себя на краях? Как они ведут себя в середине?\n",
    "\n",
    "__Ответ:__ \n",
    "\n",
    "5. Перезапустите код при $n=100$. Насколько драматично изменилась картинка? \n",
    "\n",
    "__Ответ:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1f498",
   "metadata": {},
   "source": [
    "Что почитать:\n",
    "\n",
    "- [Статья Денга про дельта-метод](https://alexdeng.github.io/public/files/kdd2018-dm.pdf)\n",
    "- [Выводим распределение для дисперсии и строим асимптотические доверительные интервалы](https://www.stat.umn.edu/geyer/s06/5102/notes/ci.pdf) (тут же есть про стабилизацию дисперсии и многое другое)\n",
    "- [Доверительный интервал Уилсона](https://www.econometrics.blog/post/the-wilson-confidence-interval-for-a-proportion/)\n",
    "- [Доверительный интервал агрести-коула](https://www.econometrics.blog/post/don-t-use-the-textbook-ci-for-a-proportion/)\n",
    "- [Сравниваем разные доверительные интервалы для долей](https://www.ime.usp.br/~jmsinger/Textos/Castroetal2019.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ba3af",
   "metadata": {},
   "source": [
    "### Задача 2: квантильное преобразование (2 балла)\n",
    "\n",
    "Компьютер хорошо умеет генерировать равномерные случайные величины. Для разных популярных распределений вроде нормального люди также смогли придумать хорошие генераторы. Но что делать, если нам хочется научиться генерировать совершенно любую случайнйю величину? В этом людям помогает квантильное преобразование!\n",
    "\n",
    "#### Теорема:\n",
    "\n",
    "Пусть функция распределения $F_X(x)$ непрерывна. Тогда случайная величина $Y = F(X)$ имеет равномерное распределение на отрезке $[0; 1]$.\n",
    "\n",
    "\n",
    "#### Следствие:\n",
    "\n",
    "Пусть $Y \\sim U[0;1]$, а $F(x)$ произвольная функция распределения. Тогда случайная величина $X = F^{-1}(Y)$ будет иметь функцию распределения $F(x)$.\n",
    "\n",
    "\n",
    "#### Что это нам даёт:\n",
    "\n",
    "\n",
    "- Позволяет генерировать из равномерного распределения другие. Достаточно найти обратную функцию и вычислить её значение от каждого элемента в выборке.\n",
    "- Применимо невсегда, напрмер, для нормального распределения используют другие алгоритмы, так как функцию распределения для него невозможно записать аналитически.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/matstat-AB/main/week05_LLN_CLT/image/quant.png\" height=\"200\"> \n",
    "</center>\n",
    "\n",
    "\n",
    "__Полезные видео и код:__ \n",
    "\n",
    "- [Видео с объяснением квантильного преобразования](https://www.youtube.com/watch?v=Dxtj-3N22_A&list=PLCf-cQCe1FRyg1ajZ2HJVKknbuTujBOLN&index=9)\n",
    "- [Видео с его использованием в питоне](https://www.youtube.com/watch?v=ivpWyorfWlA&list=PLCf-cQCe1FRyg1ajZ2HJVKknbuTujBOLN&index=10)\n",
    "- [Тетрадка с кодом из видео](https://github.com/FUlyankin/matstat-AB/blob/main/week05_LLN_CLT/09_python_distributions.ipynb)\n",
    "- [Лекция Фила из маги по терверу с подробным разбором квантильного преобразования и выведением функции распределения максимума](https://www.youtube.com/watch?v=vvpRREpe0Bw)\n",
    "- [Выведение функции распределения для максимума в учебнике Черновой](https://tvims.nsu.ru/chernova/tv/lec/node68.html)\n",
    "\n",
    "\n",
    "\n",
    "Пуcть у нас есть стрёмная функция распределения: \n",
    "\n",
    "$$\n",
    "F(x) = \\left( \\frac{\\ln x}{\\ln \\theta} \\right)^{\\alpha},  \\quad x \\in [1; \\theta]\n",
    "$$\n",
    "\n",
    "__а) [1 балл]__ Сгенерируйте из него выборку с помощью квантильного преобразования. Параметры $\\alpha$ и $\\theta$ возьмите на свой вкус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ff5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c9643",
   "metadata": {},
   "source": [
    "__б) [1 балл]__ Пусть случайные величины $X_1, \\ldots, X_n \\sim U[0;1].$ Пусть $Y_n = max(X_1, \\ldots, X_n).$ \n",
    "\n",
    "Мы знаем, что $F_{Y_n}(t) = x^n,$ если $x \\in [0; 1].$ \n",
    "\n",
    "Напишите функцию, которая сгенерирует с помощью квантильного преобразования выборку из распределения $F_{Y_n}(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0800fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001b408",
   "metadata": {},
   "source": [
    "### Задача 3: метрика плохих показов (13 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865a8e8",
   "metadata": {},
   "source": [
    "Мало того, что в интернете постоянно кто-то не прав, так ещё и куча спама, фейков, хейтспича, кликбейта и другого «плохого» контента. Каждая уважающая себя платформа борется с ним. Более того, гиганты вроде Facebook публикуют [transparency-отчёты](https://transparency.fb.com/reports/community-standards-enforcement/) о том, сколько показов «плохого» контента пропустила их система модерации. \n",
    "\n",
    "Давайте прежставим себе, что мы Youtube. Мы хотим, чтобы пользователям как можно реже показывался «плохой» контент. К сожалению, мы не можем сделать ручную разметку всех видео, которые загружают люди. Поток нового видео на платформе постоянно растёт. Нам надо будет поддерживать огромный штат модераторов. Поэтому для каждого нарушения с помощью машинного обучения обычно делают классификаторы. Нейросети пытаются предсказать, есть ли в видео неприемлимый контент. Если его вероятность высокая, видео автоматически банится. Если нейросеть не уверена, видео отправляют на разметку модераторам. Если мы обучим хороший классификатор, подавляющая часть потока будет оставаться без модерации. \n",
    "\n",
    "Параллельно с классификатором мы можем начать размечать модераторами жалобы и самые вирусные видосы. Машинное обучение даёт осечки. Какое-то одно «плохое» видео, пропущенное нашей системой, может набрать много показов. Постмодерация самых вирусных видео и жалоб могут нас от этого спасти. \n",
    "\n",
    "Нам хотелось бы понимать, насколько хорошо работает система модерации. Для этого мы будем оценивать долю «плохих» показов. Если мы показываем видео со спамом, такие показы мы считаем плохими.\n",
    "\n",
    "**Наша задача**  —  получить несмещённую оценку доли плохих показов на youtube, а также построить для неё доверительный интервал. Тогда мы будем видеть, с каким нарушением у нас больше всего проблем, а также сможем придумывать для системы модерации разные улучшения и понимать, насколько они эффективны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59afac5d",
   "metadata": {},
   "source": [
    "#### 1. Данные\n",
    "\n",
    "У нас есть данные о просмотре видео на youtube за сутки. Таблица весьма громадная, так как она содержит несколько миллиардов уникальных видео. Мы хотим выбрать из неё тысячу случайных видео, чтобы модераторы разметили их на спам. При этом нужно учесть частоту просмотров. Популярные видео должны иметь больше шансов попасть в подвыборку. Тогда по этой разметке мы сможем оценить долю плохих показов. \n",
    "\n",
    "|      video                         |            shows                  |\n",
    "|:----------------------------------:|:---------------------------------:|\n",
    "| Baby Shark                         |  13 840 000 000                   |\n",
    "| Despacito                          |   8 340 000 000                   |\n",
    "| Johny Johny Yes Papa               |   6 850 000 000                   |\n",
    "| ........                           |   ........                        |\n",
    "| 1. Андан-2024: симуляции           |   2                               |\n",
    "\n",
    "Давайте посчитаем для каждого видео вероятность, что оно будет показно и сделаем `np.random.choice`, как в коде ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2a6b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e011d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80., 28., 38., ...,  4.,  7.,  4.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгенерируем показы из распределения Парето, чтобы было побольше выбросов как в жизни.\n",
    "n_obs = 10**6\n",
    "\n",
    "b = 0.7\n",
    "rv = sts.pareto(b)\n",
    "shows = np.round(rv.rvs(n_obs))\n",
    "shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc932538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      1,       2,       3, ...,  999998,  999999, 1000000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos = np.arange(1, n_obs + 1) # это id видео\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89f6421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.18457044e-07, 4.14599655e-08, 5.62670960e-08, ...,\n",
       "       5.92285221e-09, 1.03649914e-08, 5.92285221e-09])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = shows / shows.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21db5060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([768271, 812217, 414834, 165312, 170225, 395504, 308242, 545462,\n",
       "       446027, 238983, 129475, 965156, 417091, 860999, 342125, 925607,\n",
       "        43561, 917140, 286172,  49728,  15892, 402615, 644579, 302866,\n",
       "       747396, 838758, 702797, 857377,  51794, 585780,  14362, 428344,\n",
       "       758926, 201559, 273271, 444867, 665524, 770342, 520396, 947914,\n",
       "       288471, 360643, 611616, 537334, 682029, 498533, 682266, 206493,\n",
       "       383007, 948874, 812276, 230982, 647792, 374799, 898154, 878963,\n",
       "       851828, 547177, 494730, 378140, 347122, 199006, 542640, 363031,\n",
       "       965789, 906257, 959535, 912033, 158999, 451906, 645412, 913439,\n",
       "       894606, 810839, 988846, 658025, 277031, 189370,  85941, 242020,\n",
       "       998863,  75376, 821560,  56462, 764648, 720647, 287435, 697870,\n",
       "       603643, 272894, 486735, 714434, 214356, 284111, 905541, 764242,\n",
       "       636401, 213231, 781121, 665910])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 100\n",
    "np.random.choice(videos, size=k, replace=False, p = p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021deaf1",
   "metadata": {},
   "source": [
    "У такого подхода есть проблема. Если в таблице миллиарды строк, мы не сможем сохранить таблицу в оперативную память. Нам для генерации выборки понадобится супер-компьютер. Хотелось бы этого избежать. К счастью, для решения этой проблемы есть [много алгоритмов,](https://en.wikipedia.org/wiki/Reservoir_sampling) и мы с вами реализуем один из них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea21d10",
   "metadata": {},
   "source": [
    "#### 2. Сэмплирование с повторениями\n",
    "\n",
    "Для начала поработаем с сэмплированием с повторениями. Сгенерируем таблицу с данными. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9387a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>shows</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  shows  is_spam\n",
       "0         1    4.0        0\n",
       "1         2    2.0        0\n",
       "2         3    3.0        0\n",
       "3         4   33.0        0\n",
       "4         5    5.0        0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs = 10**5\n",
    "\n",
    "b = 0.7\n",
    "rv = sts.pareto(b)\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    'video_id': np.arange(1, n_obs + 1),\n",
    "    'shows': np.round(rv.rvs(n_obs)),\n",
    "    'is_spam': np.random.binomial(1, 0.01, n_obs), # пусть в 1% видео встречается спам\n",
    "})\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12344e8",
   "metadata": {},
   "source": [
    "__а) [1 балл]__  Посчитайте по табличке `df` истиную долю плохих показов: \n",
    "\n",
    "$$\n",
    "p_{\\text{bad}} = \\frac{\\sum_{v \\in V} \\text{show}(v) \\cdot \\text{is\\_spam}(v) }{\\sum_{v \\in V} \\text{show}(v)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e574f40",
   "metadata": {},
   "source": [
    "Мы не знаем всех меток $\\text{is\\_spam}(v)$. Мы можем позволить себе разметить маленький сэмпл, $S ⊂ V$. Каждое видео попадает к нам в сэмпл пропорционально числу его показов. Поэтому логично оценить долю плохих показов как \n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{bad}} = \\frac{1}{|\\text{S}|}\\sum_{v \\in S} \\text{is\\_spam}(v).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2210a2",
   "metadata": {},
   "source": [
    "__б) [1 балл]__ С помощью функции `np.random.choice` cделайте $10^4$ выборок __с повторениями__ размера $1000$. \n",
    "\n",
    "Постройте для каждой оценку доли плохих показов. Считайте, что модератор во время разметки безошибочно определяет значение из колонки `is_spam`. \n",
    "\n",
    "Нарисуйте гистограмму для получившегося распределения. Отметьте на ней настоящую долю плохих показов и получившееся у вас среднее. Правда ли, что мы получили несмещённую оценку?\n",
    "\n",
    "**Hint:** Перейдите от `pandas` к `numpy` и обратите внимание на команду `argsort()`. Такой код будет работать быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eacebab",
   "metadata": {},
   "source": [
    "#### 3. Сэмплирование без повторений\n",
    "\n",
    "Выше мы сказали, что делать `np.random.choice` для огромных таблиц невозможно, так как они не поместятся в оперативную память. \n",
    "\n",
    "Более того, нам надо дать модераторам разметить видео на спам. Многие видео будут повторяться. Уникальных видео каждый раз будет разное количество. Нагрузка на модераторов будет неравномерной. Они будут жаловаться на это. Хочется, чтобы нагрузка всегда была одинаковой.\n",
    "\n",
    "Давайте сменим подход и будем делать **случайную взвешенную выборку БЕЗ ПОВТОРЕНИЙ.** Это означает, что наблюдения зависят друг от друга. Причём корреляция между ними довольно высокая. Это будет приводить к проблемам. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcc751",
   "metadata": {},
   "source": [
    "Пусть у нас есть $n$ объектов. Мы хотим отобрать $k$ видео для разметки модераторами. Использование `np.random.choice` можно проинтерпретировать следующим образом: \n",
    "\n",
    "1. Напишем название $i-$го видео на разных табличках $\\text{shows}_i$ раз.\n",
    "2. Случайно перемешаем все таблички и положим их в стопку в случайном порядке.\n",
    "3. Будем отбирать самые верхние таблички до тех пор, пока не встретим $k$ разных названий видео."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d92a13",
   "metadata": {},
   "source": [
    "Если бы нам были бы не важны веса в виде показов, каждое видео попадало бы к нам в выборку равновероятно. Мы могли бы отобрать выборку размера $k$ следущим образом: \n",
    "\n",
    "1. Для каждого видео генерируем $X_i \\sim U[0; 1]$ (по одной табличке на видео).\n",
    "2. Сортируем все видео по сгенерированной случайной величине.\n",
    "3. Срезаем топ-k видео в итоговую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461280ff",
   "metadata": {},
   "source": [
    "Веса в виде показов для нас важны. В теринах равномерных случайных величин алгоритм с весами можно записать так: \n",
    "\n",
    "1. Для $i$-го видео генерируем $\\text{shows}_i$ независимых равномерных случайных величин (для каждого видео свое число табличек). \n",
    "2. Складываем все таблички в одну стопку и сортируем их по сгенерированным величинам.\n",
    "3. Идём по массиву от больших элементов к меньшим и отбираем видео, пока не накопим $k$ элементов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e24fc7",
   "metadata": {},
   "source": [
    "Очень не хочется для какого-нибудь популярного видео генерировать несколько миллиардов случайных чисел. Конечно же, сразу нужно генерировать случайную величину, которая будет максимумом нескольких независимых равномерных случайных величин. На итоговую сортировку это никак не повлияет. Мы отберём те же самые видео.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90d075",
   "metadata": {},
   "source": [
    "Сгенерировать для каждого видео $X_{i,max}$ можно с помощью квантильного преобразования. \n",
    "\n",
    "Пусть случайные величины $X_1, \\ldots, X_m \\sim \\text{iid} \\, U[0;1].$ Пусть $Y = \\max(X_1, \\ldots, X_m).$ \n",
    "\n",
    "Мы знаем, что $F_{Y}(t) = x^m,$ если $x \\in [0; 1].$ Выборку из распределения $F_{Y}(t)$ можно сгенерировать в два шага: \n",
    "\n",
    "- $x_1, \\ldots, x_m \\sim \\text{iid} \\, U[0;1]$\n",
    "- $y_i = x_i^{\\frac{1}{m}}$\n",
    "\n",
    "__в) [1 балл]__ Фактически нам надо отсортировать все видео по величине $X_i^{\\frac{1}{\\text{shows}_i}}$, где $X_i \\sim U[0;1]$, и отобрать топ-$k$ видео в выборку для разметки модераторами. Сделайт, используя эту процедуру, для таблицы `df` сэмпл размера $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c245f",
   "metadata": {},
   "source": [
    "__г) [1 балл]__ Вычислять на компьютере корни из числа, лежащего между 0 и 1 не очень удобно с точки зрения округления.\n",
    "\n",
    "Гораздо эффективнее отобрать топ по величине $\\frac{1}{\\text{shows}_i} \\cdot \\ln X_i$, где $X_i \\sim U[0;1]$. Проделайте это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd49407",
   "metadata": {},
   "source": [
    "Такую процедуру можно найти в проде у многих компаний. В Яндексе похожая процедура используется для сэмплирования поисковых запросов для дальнейшей разметки. [В статье](https://yadi.sk/i/IxKLPFEj3TSpPe) можно найти доказательство того, что алгоритм даст корректную вероятность для каждого видео.\n",
    "\n",
    "В примере выше мы держим табличку в оперативной памяти компьютера. Это игрушечный пример и это позволительно. В реальной жизни мы можем считывать строки с жесткого диска, для каждой из них по очереди генерировать случайную величину и поддерживать топ-к уникальных видео в памяти. Например, в этом может помочь такая структура данных [как куча.](https://ru.wikipedia.org/wiki/Куча_(структура_данных)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c725136",
   "metadata": {},
   "source": [
    "#### 4. Оценка доли плохих показов\n",
    "\n",
    "Дальше нам остаётся разметить видео и аккуратно посчитать итоговую метрику. Беда будет в том, что она окажется смещённой. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24fabe",
   "metadata": {},
   "source": [
    "__д) [1 балл]__ Сделайте $10^4$ выборок размера $1000$. Постройте для каждой оценку доли плохих показов. Считайте, что модератор во время разметки безошибочно определяет значение из колонки `is_spam`. \n",
    "\n",
    "Нарисуйте гистограмму для получившегося распределения. Отметьте на ней настоящую долю плохих показов и получившееся у вас среднее. Правда ли, что они сильно отличаются друг от друга? Найдите среднее смещение оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff992280",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b820f32",
   "metadata": {},
   "source": [
    "**Откуда появляется это смещение?** Представим себе, что у нас есть случайная величина $X$, которая принмает пять значений с вероятностями \n",
    "\n",
    "<center>\n",
    "\n",
    "|  $X$         | $x_1$   | $x_2$ | $x_3$ | $x_4$    | $x_5$    |\n",
    "|:------------:|:-------:|:-----:|:-----:|:--------:|:--------:|\n",
    "| $P(X = x)$   | $^1/_2$ |$^1/_4$|$^1/_8$|$^1/_{16}$|$^1/_{16}$|\n",
    "\n",
    "</center>\n",
    "\n",
    "Если мы делаем выборку с повторениями, как часто туда будет попадать элемент $x_1$? Элемент попадает в выборку с вероятностью $^1/_2.$ Будем вытаскивать из выборки элементы до тех пор, пока $x_1$ не окажется в наших руках. Номер попытки, начиная с которой $x_1$ окажется у нас, имеет геометрическое распределение. Если $Y \\sim \\text{Geom}(p),$ тогда $\\mathbb{E}(Y) = \\frac{1}{p}.$ \n",
    "\n",
    "Получается, элемент $x_1$ окажется в нашей выборке в среднем на второй попытке. Учитывая, что мы делаем выборку с повторениями, каждый второй элемент в ней будет принимать значение $x_1,$  каждый четвёртый будет принимать значение $x_2$, каждый восьмой будет принимать значение $x_3$ и так далее. Обратим внимание, что если мы делаем выборку из трёх элементов без повторений, то чаще всего мы будем работать с выборкой $x_1, x_2, x_3$. \n",
    "\n",
    "В ситуации с видео, мы сэмплируем их пропорционально числу показов. В числе показов может быть очень сильный перекос. Какие-то видео показываются в рекомендательной системе десятки раз, а какие-то вирусятся и прорываются в тренды. Каждый раз, когда мы без возвращения берём новое видео в выборку, мы занижаем его присутствие в итоговой выборке.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290bb25",
   "metadata": {},
   "source": [
    "#### 5. Исправляем смещение\n",
    "\n",
    "На выборку без повторений размера $n$ можно смотреть следующим образом: мы генерируем выборку с возвращением до тех пор, пока количество уникальных элементов не достигнет числа $n$. При этом, если мы берём на каком-то шаге элемент уже выбранный ранее, мы не включаем его в выборку, а только отдельно запоминаем где-нибудь счётчик числа вхождений этого элемента $c_i$. При таком подходе несмещённую оценку доли плохого можно записать как \n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{bad}} = \\frac{\\sum_{i=1}^{n} c_i \\cdot \\text{is\\_spam}(v_i)}{\\sum_{i=1}^{n} c_i.}\n",
    "$$\n",
    "\n",
    "Если выборка имеет сильно неравномерные веса, то нам придётся довольно долго генерировать элементы с возвращением, пока мы наберём необходимое количество уникальных. \n",
    "\n",
    "Поэтому вместо того, чтобы накапливать счётчики вхождений, мы рассчитаем их математическое ожидание по всем сгенерированным выборкам без повторений, имеющим такое же упорядоченное множество элементов $w_i = \\mathbb{E}(c_i).$\n",
    "\n",
    "Пусть $q_i = \\frac{show(v_i)}{\\sum_{j=1}^{|V|} show(v_j)}$,пусть $w_i^k$ —  количество вхождений элемента с индексом $i$ к моменту, когда в выборке набралось $k$ уникальных элементов. Пусть уникальные элементы попадают в выборку в порядке $v_1, v_2, v_3, \\ldots, v_n$. \n",
    "\n",
    "Когда мы возьмём первый элемент: \n",
    "\n",
    "$$\n",
    "w_1^1 =1, \\quad w_2^1 = 0, \\quad w_3^1 = 0, \\quad \\ldots, \\quad w_n^1 = 0.\n",
    "$$\n",
    "\n",
    "Прежде, чем мы достанем второй уникальный элемент, мы в среднем достанем первый элемент ещё  $\\frac{1}{1 - q_1} - 1 = \\frac{q_1}{1 - q_1}$ раз. \n",
    "\n",
    "Эту величину мы посчитали с помощью геометрического распределения. В качестве успешного события мы рассматриваем второй уникальный элемент. Вероятность успеха равна $1- q_1.$ Геометрическая случайная величина представляет из себя номер первого успешного события. Если мы хотим получить число не успешных событий, надо вычесть единицу.\n",
    "\n",
    "Получаем\n",
    "\n",
    "$$\n",
    "w_1^2 = 1 + \\frac{q_1}{1 - q_1}, \\quad w_2^2 = 1, \\quad w_3^2 = 0, \\quad \\ldots, \\quad w_n^2 = 0.\n",
    "$$\n",
    "\n",
    "Прежде, чем мы достанем третий уникальный элемент, мы в среднем достанем первые два элемента ещё $\\frac{1}{1 - (q_1 + q_2)} - 1 = \\frac{q_1 + q_2}{1 - (q_1 + q_2)}$ раз. Из них доля доставания первого элемента составляет $\\frac{q_1}{q_1 + q_2}$, а доля второго $\\frac{q_2}{q_1 + q_2}$. \n",
    "\n",
    "Получаем \n",
    "\n",
    "$$\n",
    "w_1^3 = 1 + \\frac{q_1}{1 - q_1} + \\frac{q_1}{1 - (q_1 + q_2)}, \\quad w_2^3 = 1 + \\frac{q_2}{1 - (q_1 + q_2)}, \\quad  w_3^3 = 1, \\quad \\ldots, \\quad w_n^3 = 0.\n",
    "$$\n",
    "\n",
    "Продолжая эту логику до шага $n$ получаем формулу \n",
    "\n",
    "$$\n",
    "c_i = w_i^n = 1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k} \\right).\n",
    "$$\n",
    "\n",
    "Подставим эти веса вместо счетчиков $c_i$. Это даст нам несмещённую оценку доли «пдохих» показов на основе разметки ровно $n$ объектов\n",
    "\n",
    "$$\n",
    "\\hat p_{\\text{bad}} = \\frac{\\sum_{i=1}^n \\left[ 1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k} \\right) \\right] \\cdot has\\_spam(v_i)}{\\sum_{i=1}^n \\left[ 1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k} \\right)  \\right]},  \\quad q_i = \\frac{show(v_i)}{\\sum_{j=1}^{|V|} show(v_j)}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13032a",
   "metadata": {},
   "source": [
    "__е) [1 балл]__ Сделайте $1000$ выборок без повторений размера $1000$. Постройте для каждой оценку доли плохих показов. Убедитесь, что оценка предложенная выше окажется несмещённой.\n",
    "\n",
    "**Hint:** для расчёта весов удобно воспользоваться несколько раз функцией `np.cumsum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c389472",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4c847",
   "metadata": {},
   "source": [
    "#### 6. Доверительный интервал \n",
    "\n",
    "Про долю плохих заказов надо уметь делать выводы. Для этого надо построить доверительный интервал. Для него нужна дисперсия. Все карточки размечаются модераторами незвисимо друг от друга. Получается, что дисперсию можно найти как \n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat p_{\\text{bad}}) = \\frac{\\sum_{i=1}^n c^2_i \\cdot \\text{Var}(\\text{is\\_spam}(v_i))}{\\left(\\sum_{i=1}^n c_i\\right)^2} = \\frac{\\sum_{i=1}^n c^2_i}{\\left(\\sum_{i=1}^n c_i\\right)^2} \\cdot p_{\\text{bad}} \\cdot (1 - p_{\\text{bad}})\n",
    "$$\n",
    "\n",
    "Видно, что если сэмпл занимает небольшую долю выборки и показы распределены между элементами достаточно равномерно, то веса $c_i$ будут несильно отличаться от $1$. Тогда асимптотически по мере роста размера сэмпла стандартное отклонение метрики будет падать как $\\frac{1}{\\sqrt{n}}.$ \n",
    "\n",
    "Однако, если существенная доля показов в рекомендательной ленте представлена небольшой группой видосов, тогда веса будут существенно различаться и начиная с определённого момента увеличение размера выборки не будет давать заметного снижения разброса в оценке доли плохих показов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025111a",
   "metadata": {},
   "source": [
    "__ё) [1 балл]__ Сделайте $1000$ выборок без повторений размера $1000$. Постройте по каждой $95\\%$ доверительный интервал для доли плохих показов. \n",
    "\n",
    "Убедитесь, что он действительно покрывает долю плохих показов с вероятностью $0.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeef1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a15e7f",
   "metadata": {},
   "source": [
    "__ж) [1 балл]__ Если вы всё сделали всё верно, выше симуляции дали очень плохой результат. Доверительный интервал развалился. Он оказался слишком широким. Более того, он пробивает слева ноль. \n",
    "\n",
    "Сконструируйте доверительный интервал Уилсона. Убедитесь, что он больше не пробивает отрезок $[0;1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc15cc",
   "metadata": {},
   "source": [
    "Доверительный интервал Уилсона всё еще не будет давать нужного уровня значимости из-за огромной ширины. Нам нужен какой-то способ уменьшить дисперсию. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368a66c",
   "metadata": {},
   "source": [
    "#### 7. Уменьшение дисперсии с помощью машинного обучения\n",
    "\n",
    "Когда доля маленькая, дисперсия оценки доли будет довольно высокой. Относительная ошибка в оценке быстро растёт при уменьшении доли размечаемых видосов, содержащих спам\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma}{p_{\\text{bad}}} \\approx \\frac{\\sqrt{ p_{\\text{bad}} \\cdot (1 -  p_{\\text{bad}})/n} }{ p_{\\text{bad}} } \\approx \\frac{1}{\\sqrt{n \\cdot p_{\\text{bad}}}}.\n",
    "$$\n",
    "\n",
    "Например, если реальная доля показов, содержащих спам составляет $0.5\\%,$ то при разметке выборки из $1000$ видео стандартное отклонение составить $0.22\\%$ и положительную метку будут обычно получать $3-7$ видео, а наша оценка доли спама будет колебаться в пределах $(0.5 \\pm 0.22)\\%.$ При таком уровне шума отслеживать эффект от введения различных улучшений в пайплайнах модерации становится практически невозможно. \n",
    "\n",
    "Однако, если бы у нас существовал способ повысить долю просэмплированных видео с положительной разметкой в сто раз, то мы бы получили оценку доли плохого в сэмпле равную $(50 \\pm 1.6)\\%.$ \n",
    "\n",
    "Принимая во внимание, что при построении сэмпла мы завысили долю плохого в сто раз, получаем, что реальная доля плохого составит  $(0.5 \\pm 0.016)\\%.$ То есть с помощью приоритизации мы могли бы снизить разброс примерно в $14$ раз. \n",
    "\n",
    "На практике мы не можем заранее угадать какие видео будут размечены как плохие. Однако у нас есть ML-модели, предсказываютщие подозрительность видео. Например, вероятность того, что видос относится к плохому классу, $\\text{score}(v_i)$.\n",
    "\n",
    "Для того, чтобы сделать размечаемую выборку более представительной по классам, но при этом сохранить репрезентативность по потоку, повысим вероятность сэмплирования видео с высоким уровнем подозрительности.\n",
    "\n",
    "Чтобы после этого по разметке сэмпла оценить долю плохого на потоке, нам нужно обратно изменить веса видео, чтобы величины соотвествовали ожидаемым значениям без перевзвешивания. Новые веса в формуле будут равны \n",
    "\n",
    "$$\n",
    "c_i = \\sum_{i=1}^n  \\frac{1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k}\\right)}{\\text{score}(v_i)}, \\quad q_i = \\frac{\\text{show}(v_i)}{\\sum_{j=1}^{|V|} \\text{show}(v_j)}\n",
    "$$\n",
    "\n",
    "Оценка доли будет искаться как \n",
    "\n",
    "$$\n",
    "\\hat p_{spam} = \\frac{\\sum_{i=1}^n c_i \\cdot \\text{is\\_spam}(v_i)}{\\sum_{i=1}^n c_i}.\n",
    "$$\n",
    "\n",
    "Аналогично дисперсия такой оценки будет иметь вид \n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat p_{\\text{bad}}) = \\frac{\\sum_{i=1}^n c^2_i \\cdot \\text{Var}(\\text{is\\_spam}(v_i))}{\\left(\\sum_{i=1}^n c_i\\right)^2}.\n",
    "$$\n",
    "\n",
    "В формуле дисперсии $\\text{Var}(\\text{is\\_spam}(v_i))$ уже нельзя считать одинаковыми, так как есть зависимость между весом объекта, обусловленным $score(v_i)$ и дисперсией бернулиевской случайной величины $\\text{is\\_spam}(v_i).$ \n",
    "\n",
    "Если предсказывающая модель обучена в точности на реальном распределении, наблюдаемом в потоке либо [откалибрована,](https://github.com/esokolov/ml-course-hse/blob/master/2022-fall/seminars/sem06-calibration.ipynb) то можно считать, что $\\text{score}(v_i) = \\mathbb{P}(\\text{is\\_spam}(v_i) = 1).$ \n",
    "\n",
    "Тогда \n",
    "\n",
    "$$\n",
    "\\text{Var}(\\text{is\\_spam}(v_i)) = \\mathbb{P}(\\text{is\\_spam}(v_i) = 1) \\cdot \\mathbb{P}(\\text{is\\_spam}(v_i) = 0) = \\text{score}(v_i) \\cdot (1 - \\text{score}(v_i)).\n",
    "$$\n",
    "\n",
    "Если откалибровать модель не представляется возможным,тогда можно оценить математическое ожидание и дисперсию элемента на основе исторических данных по разметке элементов с похожими предсказаниями модели. \n",
    "\n",
    "Если для какого-то объекта по каким-то причинам отсутствует $\\text{score}(v_i),$ то при сэмплировании мы можем вставить ему произвольный вес и использовать верхнюю оценку на дисперсию бернуллиевской случайной величины, $\\text{Var}(\\text{is\\_spam}(v_i)) \\le 0.25.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14705fcb",
   "metadata": {},
   "source": [
    "__з) [1 балл]__  Переделайте процесс генерации данных таким образом, чтобы для них можно было обучить классификатор. С помощью любых функций из sklearn сгенерируйте датасет таким образом, чтобы спам в данных встречался в $1\\%$ случаев. Обучите логистическую регрессию. Генерируйте датасет таким образом, чтобы качество модели по метрике roc-auc оказалось в районе 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a949c45",
   "metadata": {},
   "source": [
    "__и) [1 балл]__ Сделайте $1000$ выборок без повторений размера $1000$. Постройте по каждой $95\\%$ доверительный интервал для доли плохих показов. \n",
    "\n",
    "Правда ли доверительный интервал стал уже? Правда ли, что он покрывает долю плохих показов с вероятностью $0.95$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ab0fd",
   "metadata": {},
   "source": [
    "__к) [1 балл]__ Проделайте то же самое для доверительного интервала Уилсона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a532b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58790683",
   "metadata": {},
   "source": [
    "__л) [1 балл]__ Вместо логистической регрессии обучите SVM. Попробуйте построить доверительный интервал одним из способов, упомянутых выше. Что у вас получилось? Правда ли он покрывает долю плохих показов с вероятностью $0.95$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
    "# will the code be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8719324",
   "metadata": {},
   "source": [
    "__м) [1 балл]__ Попробуйте в настройках эксперимента поменять долю спама в выборке с $1%$ до $80\\%$. Что произойдёт с дисперсией и смещением?\n",
    "\n",
    "__Ваш ответ:__ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
